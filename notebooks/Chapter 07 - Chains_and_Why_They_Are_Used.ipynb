{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2007%20-%20Chains_and_Why_They_Are_Used.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAsSxpAyUOnF",
        "outputId": "89d0a63f-9c88-4a8f-958a-e97c7451a9f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-J9LsngZsfp"
      },
      "source": [
        "# Calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44NeYe0GXe32"
      },
      "source": [
        "## __ call __"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQn4558HUPvI",
        "outputId": "35edbf54-a44e-41d7-806f-2d2e0f374970"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (4.1.6) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\openai.py:179: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "c:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\openai.py:753: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'word': 'artificial', 'text': 'synthetic'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import PromptTemplate, OpenAI, LLMChain\n",
        "\n",
        "prompt_template = \"What is a word to replace the following: {word}?\"\n",
        "\n",
        "# Set the \"OPENAI_API_KEY\" environment variable before running following line.\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate.from_template(prompt_template)\n",
        ")\n",
        "llm_chain(\"artificial\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lGTqvJxXjMZ"
      },
      "source": [
        "## Apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhrI8CggVtJo",
        "outputId": "f2673059-65b2-4ba4-df2c-063688277c88"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "OpenAIChat currently only supports single prompt, got ['What is a word to replace the following: artificial?', 'What is a word to replace the following: intelligence?', 'What is a word to replace the following: robot?']",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m input_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martificial\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintelligence\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      4\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrobot\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      5\u001b[0m ]\n\u001b[1;32m----> 7\u001b[0m \u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\chains\\llm.py:186\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    185\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    187\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)\n\u001b[0;32m    188\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs})\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\chains\\llm.py:183\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    178\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    179\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    180\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_list},\n\u001b[0;32m    181\u001b[0m )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 183\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    185\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\chains\\llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    134\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    138\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    139\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\base.py:206\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    205\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    207\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager:\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\base.py:198\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    194\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m), prompts, invocation_params\u001b[38;5;241m=\u001b[39mparams, options\u001b[38;5;241m=\u001b[39moptions\n\u001b[0;32m    195\u001b[0m )\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 198\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    203\u001b[0m     )\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    205\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\openai.py:790\u001b[0m, in \u001b[0;36mOpenAIChat._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    785\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m--> 790\u001b[0m     messages, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_chat_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    791\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\openai.py:769\u001b[0m, in \u001b[0;36mOpenAIChat._get_chat_params\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_chat_params\u001b[39m(\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompts: List[\u001b[38;5;28mstr\u001b[39m], stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    767\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    770\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAIChat currently only supports single prompt, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    771\u001b[0m         )\n\u001b[0;32m    772\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix_messages \u001b[38;5;241m+\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompts[\u001b[38;5;241m0\u001b[39m]}]\n\u001b[0;32m    773\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params}\n",
            "\u001b[1;31mValueError\u001b[0m: OpenAIChat currently only supports single prompt, got ['What is a word to replace the following: artificial?', 'What is a word to replace the following: intelligence?', 'What is a word to replace the following: robot?']"
          ]
        }
      ],
      "source": [
        "input_list = [\n",
        "    {\"word\": \"artificial\"},\n",
        "    {\"word\": \"intelligence\"},\n",
        "    {\"word\": \"robot\"}\n",
        "]\n",
        "\n",
        "llm_chain.apply(input_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5jB5LBJXk9s"
      },
      "source": [
        "## Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYi0o5KqV68n",
        "outputId": "e656827f-2299-423e-f4a8-af09a1402fae"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "OpenAIChat currently only supports single prompt, got ['What is a word to replace the following: artificial?', 'What is a word to replace the following: intelligence?', 'What is a word to replace the following: robot?']",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\chains\\llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    134\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    138\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    139\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\base.py:206\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    205\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    207\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager:\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\base.py:198\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    194\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m), prompts, invocation_params\u001b[38;5;241m=\u001b[39mparams, options\u001b[38;5;241m=\u001b[39moptions\n\u001b[0;32m    195\u001b[0m )\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 198\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    203\u001b[0m     )\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    205\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\openai.py:790\u001b[0m, in \u001b[0;36mOpenAIChat._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    785\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m--> 790\u001b[0m     messages, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_chat_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    791\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n",
            "File \u001b[1;32mc:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\openai.py:769\u001b[0m, in \u001b[0;36mOpenAIChat._get_chat_params\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_chat_params\u001b[39m(\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompts: List[\u001b[38;5;28mstr\u001b[39m], stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    767\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    770\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAIChat currently only supports single prompt, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    771\u001b[0m         )\n\u001b[0;32m    772\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix_messages \u001b[38;5;241m+\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompts[\u001b[38;5;241m0\u001b[39m]}]\n\u001b[0;32m    773\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params}\n",
            "\u001b[1;31mValueError\u001b[0m: OpenAIChat currently only supports single prompt, got ['What is a word to replace the following: artificial?', 'What is a word to replace the following: intelligence?', 'What is a word to replace the following: robot?']"
          ]
        }
      ],
      "source": [
        "llm_chain.generate(input_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks4ej9ZXXm-E"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q1BvtRiXuLg"
      },
      "source": [
        "#### Multiple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OX4RrRrlXvWm",
        "outputId": "c9cc9d8a-1585-4f6d-ca5f-e752c04c1fd2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nVentilator'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template = \"Looking at the context of '{context}'. What is a approapriate word to replace the following: {word}?\"\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate(template=prompt_template, input_variables=[\"word\", \"context\"]))\n",
        "\n",
        "llm_chain.predict(word=\"fan\", context=\"object\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NpdjaLWwYQQ1",
        "outputId": "5ebe6455-95a2-4167-e235-d49982f9fe0b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nAdmirer'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_chain.predict(word=\"fan\", context=\"humans\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9P5gGwxClJeL",
        "outputId": "560ea2b2-b540-42a8-c18a-19fcca6f0e70"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nVentilator'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# llm_chain.run(word=\"fan\", context=\"object\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNPOT6iAbt1l"
      },
      "source": [
        "### from string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6T5_9k2bx_N"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Looking at the context of '{context}'. What is a approapriate word to replace the following: {word}?\"\"\"\n",
        "llm_chain = LLMChain.from_string(llm=llm, template=template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AkE6wx8Vb9Ns",
        "outputId": "dbedb888-49d7-43da-88a5-05472fbea85d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nVentilator'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_chain.predict(word=\"fan\", context=\"object\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRIaIXSKZu6U"
      },
      "source": [
        "# Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aIEZWDQtZwKw",
        "outputId": "aa45ef77-7ea6-42ea-c613-94ac0accfb88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'synthetic, man-made, fake, simulated, imitation, faux, ersatz, fabricated, counterfeit, pseudo'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\"\"\"\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate(template=template, input_variables=[], output_parser=output_parser))\n",
        "\n",
        "llm_chain.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ18G38-aXcE",
        "outputId": "248f1533-6948-4f94-be8d-1ceb88433c20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\chains\\llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['synthetic',\n",
              " 'man-made',\n",
              " 'fake',\n",
              " 'simulated',\n",
              " 'imitation',\n",
              " 'faux',\n",
              " 'counterfeit',\n",
              " 'ersatz',\n",
              " 'fabricated',\n",
              " 'manufactured']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_chain.predict_and_parse()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b8_oGlAcx3F"
      },
      "source": [
        "# Conversational Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTYpcUktae6w",
        "outputId": "3c7c83dd-5ba6-44f6-bbe7-baf153c97232"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['synthetic', 'man-made', 'simulated']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "\n",
        "{input}\"\"\"\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
        "    memory=ConversationBufferMemory())\n",
        "\n",
        "conversation.predict_and_parse(input=\"Answer briefly. write the first 3 options.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7_vgaBgeWWG",
        "outputId": "0e9c50d3-a48f-4083-bafe-4944a53dda26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Fabricated', 'Simulated', 'Automated', 'Constructed']"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict_and_parse(input=\"And the next 4?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kz12V6FhjC3"
      },
      "source": [
        "# Debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGRoPJ1xhtTE",
        "outputId": "4bfd9840-47a9-4967-8c9d-e6a9ff0fbaa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'artificial' as comma separated.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "\n",
            "Answer briefly. write the first 3 options.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Synthetic', 'Manufactured', 'Imitation']"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
        "    memory=ConversationBufferMemory(),\n",
        "    verbose=True)\n",
        "\n",
        "conversation.predict_and_parse(input=\"Answer briefly. write the first 3 options.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XI9e40ui1yX"
      },
      "source": [
        "# Sequential Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A16wajt2hxLE"
      },
      "outputs": [],
      "source": [
        "# from langchain.chains import SimpleSequentialChain\n",
        "# overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs4Chc0iKaj3"
      },
      "source": [
        "# Custom Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3tCjI4DtKbTG"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.base import Chain\n",
        "\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "class ConcatenateChain(Chain):\n",
        "    chain_1: LLMChain\n",
        "    chain_2: LLMChain\n",
        "\n",
        "    @property\n",
        "    def input_keys(self) -> List[str]:\n",
        "        # Union of the input keys of the two chains.\n",
        "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
        "        return list(all_input_vars)\n",
        "\n",
        "    @property\n",
        "    def output_keys(self) -> List[str]:\n",
        "        return ['concat_output']\n",
        "\n",
        "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
        "        output_1 = self.chain_1.run(inputs)\n",
        "        output_2 = self.chain_2.run(inputs)\n",
        "        return {'concat_output': output_1 + output_2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-W3ZqALLbwP",
        "outputId": "83f65c01-5573-403f-9180-7a2b60a41b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated output:\n",
            "Artificial means made or produced by human beings rather than occurring naturally. It can also refer to something that is not genuine or real.Synthetic\n"
          ]
        }
      ],
      "source": [
        "prompt_1 = PromptTemplate(\n",
        "    input_variables=[\"word\"],\n",
        "    template=\"What is the meaning of the following word '{word}'?\",\n",
        ")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
        "\n",
        "prompt_2 = PromptTemplate(\n",
        "    input_variables=[\"word\"],\n",
        "    template=\"What is a word to replace the following: {word}?\",\n",
        ")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
        "\n",
        "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
        "concat_output = concat_chain.run(\"artificial\")\n",
        "print(f\"Concatenated output:\\n{concat_output}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPESOSrhtfDiEeFVbO8r7kg",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "data-science-project-VmJyb5kE-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
