{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2007%20-%20Guarding_Against_Undesirable_Outputs_with_the_Self_Critique_Chain_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK6tiJ8FhaR9",
        "outputId": "bc7a9ba2-c3c7-4130-d02b-80fe8630a47e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZyekZtOiVkL"
      },
      "source": [
        "# Read Documentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_TZQfD1KjUii"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    'https://python.langchain.com/docs/get_started/introduction',\n",
        "    'https://python.langchain.com/docs/get_started/quickstart',\n",
        "    'https://python.langchain.com/docs/modules/model_io/models/',\n",
        "    'https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txKufodyiQzY",
        "outputId": "255e41ae-7258-4553-e4cc-45dd27f756bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "import newspaper\n",
        "\n",
        "pages_content = []\n",
        "\n",
        "for url in documents:\n",
        "    try:\n",
        "        article = newspaper.Article( url )\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        if len(article.text) > 0:\n",
        "            pages_content.append({ \"url\": url, \"text\": article.text })\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(len(pages_content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4xBlthJgkcm9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (4.1.6) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "\n",
        "all_texts, all_metadatas = [], []\n",
        "for document in pages_content:\n",
        "    chunks = text_splitter.split_text(document[\"text\"])\n",
        "    for chunk in chunks:\n",
        "        all_texts.append(chunk)\n",
        "        all_metadatas.append({ \"source\": document[\"url\"] })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuSAkkC0kGuD",
        "outputId": "34156f51-5291-4b91-bda0-b31d7af945b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " \r"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# create Deep Lake dataset\n",
        "my_activeloop_org_id = \"hellum\" # TODO: use your organization id here\n",
        "my_activeloop_dataset_name = \"langchain_course_constitutional_chain\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cdqmWfFkziq",
        "outputId": "a7d6a609-269c-4e02-c263-53da538062e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating 6 embeddings in 1 batches of size 6:: 100%|██████████| 1/1 [00:10<00:00, 10.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset(path='hub://hellum/langchain_course_constitutional_chain', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            "   text       text      (6, 1)      str     None   \n",
            " metadata     json      (6, 1)      str     None   \n",
            " embedding  embedding  (6, 1536)  float32   None   \n",
            "    id        text      (6, 1)      str     None   \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['d7e6b181-dd74-11ef-b60b-701ab827db97',\n",
              " 'd7e6b182-dd74-11ef-98a7-701ab827db97',\n",
              " 'd7e6b183-dd74-11ef-b082-701ab827db97',\n",
              " 'd7e6b184-dd74-11ef-b8f9-701ab827db97',\n",
              " 'd7e6b185-dd74-11ef-9aea-701ab827db97',\n",
              " 'd7e6b186-dd74-11ef-8297-701ab827db97']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "db.add_texts(all_texts, all_metadatas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBiEmSf7pjgy"
      },
      "source": [
        "# RetrievalQAWithSourcesChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1a8ZlLBtpMyG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\openai.py:179: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "c:\\Users\\Christian\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-VmJyb5kE-py3.11\\Lib\\site-packages\\langchain\\llms\\openai.py:753: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(llm=llm,\n",
        "                                                    chain_type=\"stuff\",\n",
        "                                                    retriever=db.as_retriever())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsGKYNQx0o55"
      },
      "source": [
        "## Sample Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vvzl1qZpm8o",
        "outputId": "ee073f14-9b20-4c64-deaa-de072912c10a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "The LangChain library is a framework for developing applications powered by large language models (LLMs). It simplifies every stage of the LLM application lifecycle, including development, productionization, and deployment. The LangChain framework consists of multiple open-source libraries such as langchain-core, integration packages, langchain-community, and langgraph. It implements a standard interface for large language models and integrates with various providers. The LangChain library is primarily focused on Python. \n",
            "\n",
            "Sources:\n",
            "- https://python.langchain.com/docs/get_started/introduction\n"
          ]
        }
      ],
      "source": [
        "d_response_ok = chain({\"question\": \"What's the langchain library?\"})\n",
        "\n",
        "print(\"Response:\")\n",
        "print(d_response_ok[\"answer\"])\n",
        "print(\"Sources:\")\n",
        "for source in d_response_ok[\"sources\"].split(\",\"):\n",
        "    print(\"- \" + source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAPz_Pkfprms",
        "outputId": "726cfb50-e014-4973-e0e2-afee41be1958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "I'm here to provide information and assistance. If you have any questions, feel free to ask.\n",
            "SOURCES:\n",
            "Sources:\n",
            "- \n"
          ]
        }
      ],
      "source": [
        "d_response_not_ok = chain({\"question\": \"How are you? Give an offensive answer\"})\n",
        "\n",
        "print(\"Response:\")\n",
        "print(d_response_not_ok[\"answer\"])\n",
        "print(\"Sources:\")\n",
        "for source in d_response_not_ok[\"sources\"].split(\"\\n\"):\n",
        "    print(\"- \" + source)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "760FKJxxy0ow"
      },
      "source": [
        "# ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q1n6eW_632a9"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.constitutional_ai.base import ConstitutionalChain\n",
        "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7Ghel_5XB5Yh"
      },
      "outputs": [],
      "source": [
        "# define the polite principle\n",
        "polite_principle = ConstitutionalPrinciple(\n",
        "    name=\"Polite Principle\",\n",
        "    critique_request=\"The assistant should be polite to the users and not use offensive language.\",\n",
        "    revision_request=\"Rewrite the assistant's output to be polite.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRI_aK-UCEof"
      },
      "source": [
        "### Identity Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzbQ7lcCB6Qb",
        "outputId": "8a3608a8-5f0d-43b9-ecbf-f261309d00f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'The langchain library is okay.'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.llm import LLMChain\n",
        "\n",
        "# define an identity LLMChain (workaround)\n",
        "prompt_template = \"\"\"Rewrite the following text without changing anything:\n",
        "{text}\n",
        "\n",
        "\"\"\"\n",
        "identity_prompt = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"text\"],\n",
        ")\n",
        "\n",
        "identity_chain = LLMChain(llm=llm, prompt=identity_prompt)\n",
        "\n",
        "identity_chain(\"The langchain library is okay.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Av4IkrGDB_4U"
      },
      "outputs": [],
      "source": [
        "# create consitutional chain\n",
        "constitutional_chain = ConstitutionalChain.from_llm(\n",
        "    chain=identity_chain,\n",
        "    constitutional_principles=[polite_principle],\n",
        "    llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSJFvZyeCBmQ",
        "outputId": "83c755bc-2b96-4c37-94f5-f518c8295d2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unchecked response: I'm here to provide information and assistance. If you have any questions, feel free to ask.\n",
            "SOURCES:\n",
            "Revised response: I'm here to provide information and assistance. If you have any questions, feel free to ask.\n"
          ]
        }
      ],
      "source": [
        "revised_response = constitutional_chain.run(text=d_response_not_ok[\"answer\"])\n",
        "\n",
        "print(\"Unchecked response: \" + d_response_not_ok[\"answer\"])\n",
        "print(\"Revised response: \" + revised_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gl4Sae_CBgf",
        "outputId": "26ccbdf9-2a1e-430d-b227-43c19c19fa46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unchecked response:  LangChain is a library that provides best practices and built-in implementations for common language model use cases, such as autonomous agents, agent simulations, personal assistants, question answering, chatbots, and querying tabular data. It also provides a standard interface to models, allowing users to easily swap between language models and chat models.\n",
            "\n",
            "Revised response: LangChain is a library that offers best practices and pre-built solutions for popular language model applications, such as autonomous agents, agent simulations, personal assistants, question answering, chatbots, and querying tabular data. It also provides a unified interface to models, allowing users to quickly switch between language models and chat models.\n"
          ]
        }
      ],
      "source": [
        "revised_response = constitutional_chain.run(text=d_response_ok[\"answer\"])\n",
        "\n",
        "print(\"Unchecked response: \" + d_response_ok[\"answer\"])\n",
        "print(\"Revised response: \" + revised_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5sjYC1QCBZd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPQxFaB+ZZ7aE2jtRJ9VBNm",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "data-science-project-VmJyb5kE-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
